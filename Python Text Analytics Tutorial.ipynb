{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics in Python Using NLTK and Gensim\n",
    "\n",
    "The purpose of this notebook is to show some Text Analytics strategies that you can use with the NLTK and Gensim packages in python. In addition, the notebook was written with a plug and play application is mind. What this means is that if you have another dataset that you want to use with this notebook, you only need to change a few code cells to run the same analysis (the csv file being read in, the name of the column that contains the reviews, and the word dictionaries if doing Rule Based Categorization). Below is the table of contents for this notebook. \n",
    "\n",
    "### Introduction\n",
    "1. Reading in the Data\n",
    "\n",
    "### Pre-analysis Definitions\n",
    "1. Stemming\n",
    "2. Lemmatization\n",
    "3. Tokenization\n",
    "4. Stop Words\n",
    "\n",
    "### Data Preprocessing\n",
    "1. Preparing data for tokenization\n",
    "2. Tokenizing, Stemming/Lemmatizing, and Removing Stop Words\n",
    "3. Word Frequencies\n",
    "4. N-grams\n",
    "\n",
    "### Topic Modelling with Gensim\n",
    "1. Topic Modelling\n",
    "2. Assigning Comments to Topics\n",
    "\n",
    "### Rule Based Categorization\n",
    "2. Tagging Basics\n",
    "3. Negation and Secondary Terms\n",
    "4. Applying Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show off the functionailty of NTLK and Gensim, we will use a sample Hotel's dataset from kaggle (https://www.kaggle.com/datafiniti/hotel-reviews/). It is a list of 1,000 hotels and their reviews provided by Datafiniti's Business Database. The dataset includes hotel location, name, rating, review data, title, and username, among other fields.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we need to import the dictionaries we need and read in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import warnings\n",
    "import importlib\n",
    "# You can comment out nltk.download lines after first run\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "data = pd.read_csv('Datafiniti_Hotel_Reviews.csv') \n",
    "data = data.dropna(subset=['reviews.text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of what the reviews look like, we print out the first three comments in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of reviews: {}\\n'.format(len(data)))\n",
    "print('Example reviews: \\n')\n",
    "for i in data['reviews.text'].iloc[0:3]:\n",
    "    print(i)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Analysis Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the analysis, we must first define the following terms: **Stemming**, **Lemmatization**, and **Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process by which a word is reduced to its word stem. The purpose of stemming (and also lemmatization) is to reduce a word to its core root. Reducing a word to its core root allows a text analytics algorithm to group words with similar meaning together in order to improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ```NLTK```, there are multiple stemming algorithms available to use. For this notebook, we will be showing ```PorterStemmer``` and ```LancasterStemmer```. First, we define a stem object for both Porter and Lancaster stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how the ```PorterStemmer``` takes an example word, **stay**, and its various ending combinations, and simplifes them to the core word **stay**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Porter Stemming Example with \"stay\"')\n",
    "print('stay --> {}'.format(porter.stem(\"stay\")))\n",
    "print('stayed --> {}'.format(porter.stem(\"stayed\")))\n",
    "print('stays --> {}'.format(porter.stem(\"stays\")))\n",
    "print('staying --> {}'.format(porter.stem(\"staying\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```PorterStemmer``` is known for it's speed and simplicity, and is typically used in Information Retrieval Environments where fast recall and fetching of search queries is important. On the other hand, the Lancaster stemmer is more aggressive in stemming words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Porter Stemming Example with \"destabilize\"')\n",
    "print('destabilized --> {}'.format(porter.stem(\"destabilized\")))\n",
    "print('destabilize --> {}'.format(porter.stem(\"destabilize\")))\n",
    "print('destabilizing --> {}'.format(porter.stem(\"destabilizing\")))\n",
    "print('destabilizes --> {}'.format(porter.stem(\"destabilizes\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancaster here stems the word destabilize to **dest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lancaster Stemming Example with \"destabilize\"')\n",
    "print('destabilized --> {}'.format(lancaster.stem(\"destabilized\")))\n",
    "print('destabilize --> {}'.format(lancaster.stem(\"destabilize\")))\n",
    "print('destabilizing --> {}'.format(lancaster.stem(\"destabilizing\")))\n",
    "print('destabilizes --> {}'.format(lancaster.stem(\"destabilizes\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **PorterStemmer** and **LancasterStemmer** has disadvantages, namely that \n",
    "\n",
    "1. The words sometimes do not stem to actual words\n",
    "2. Linguistic meaning can sometimes be lost\n",
    "\n",
    "The example below with **university** and **universe** illustrates this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Porter Stemming Example with \"university\" and \"universe\"')\n",
    "print('university --> {}'.format(porter.stem(\"university\")))\n",
    "print('universal --> {}'.format(porter.stem(\"universal\")))\n",
    "print('universities --> {}'.format(porter.stem(\"universities\")))\n",
    "print('universe --> {}'.format(porter.stem(\"universe\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lancaster Stemming Example with \"university\" and \"universe\"')\n",
    "print('university --> {}'.format(lancaster.stem(\"university\")))\n",
    "print('universal --> {}'.format(lancaster.stem(\"universal\")))\n",
    "print('universities --> {}'.format(lancaster.stem(\"universities\")))\n",
    "print('universe --> {}'.format(lancaster.stem(\"universe\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the words university and universe simplify to *univers*, and difference in meaning between the two words is lost. One way to remedy this problem is with Lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, instead of getting the stem of a word, which might not be an actual word, we can lemmetize our words to make sure that the words are grouped into an actual language word. Whereas the word **\"pictures\"** might stem to something like **\"pictur\"**, in lemmatization it would shorten to **\"picture\"**. This grouping is useful for keeping the full linguistic context of a word as opposed to just keeping the root of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ```NLTK``` package we can use ```WordNetLemmatizer``` to lemmatize our words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer   \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the example of university and universe, we see that Lemmatization ensures that we retain linguistic meaning while still grouping words where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lemmatization Example with \"university\" and \"universe\"')\n",
    "print('university --> {}'.format(lemmatizer.lemmatize(\"university\")))\n",
    "print('universal --> {}'.format(lemmatizer.lemmatize(\"universal\")))\n",
    "print('universities --> {}'.format(lemmatizer.lemmatize(\"universities\")))\n",
    "print('universe --> {}'.format(lemmatizer.lemmatize(\"universe\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we decide which strategy to use when grouping similar words? When is stemming more advantagous to use when compared with lemmatization? The answer depends your application, and the importance of retaining linguistic meaning vs. having more simplified word groupings and reducing the number of features in your algorithm. If you cut off too much of the ending of a word (**overstemming**) you may have more nonsensical terms in your output and/or lose linguistic meaning. If you cut off too little from the ending of a word (**understemming**), then you may not be grouping enough similar words together and thus open your model up to potential overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What is tokenization?*\n",
    "* Tokenization is when you take a string of text, and make each word an item in a list so each word can be analyzed individually\n",
    "* **EXAMPLE:** ```\"Our experience at Rancho Valencia was absolutely perfect from beginning to end!!!!\" --> ['our','experience','at','rancho','valencia','was', 'absolutely','perfect','from', 'beginning', 'to', 'end']```\n",
    "\n",
    "*Why do we use tokenization?*\n",
    "* It is the general format used for many text analysis techniques in NTLK, including Frequency Distribution and  N-Grams, among other techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ```NTLK```, you can tokenize a string of text using ```word_tokenize```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = 'Our experience at Rancho Valencia was absolutely perfect from beginning to end'\n",
    "sample_token = nltk.word_tokenize(sample_string.lower())\n",
    "\n",
    "print('Raw Comment: \"Our experience at Rancho Valencia was absolutely perfect from beginning to end\"')\n",
    "print('Cleaned Comment: {}'.format(sample_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue that adds a lot of noise to a text analytics model are common words that appear in many comments but often add little contextual meaning. Examples of common words are \"the\", \"at\", \"a\", \"an\". These words are known as **stop words**. Fortunately, ```NTLK``` has a pre-defined corpus of stop words, ```nltk.corpus.stopwords.words('english')```, which you can you to filter out these common words from your text data. Below is a subset of words within the ```NLTK``` stop words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 10 stop words in the NLTK Stop Words Corpus (English):')\n",
    "print(nltk.corpus.stopwords.words('english')[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of applying ```nltk.corpus.stopwords.words('english')``` to your tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = 'Our experience at Rancho Valencia was absolutely perfect from beginning to end'\n",
    "sample_token = nltk.word_tokenize(sample_string.lower())   # <----- Original Tokenized Text\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "sample_token_no_stop = [word for word in sample_token if word not in stop_words] # <-- Removing Stop Words\n",
    "\n",
    "print('Raw Comment: \"Our experience at Rancho Valencia was absolutely perfect from beginning to end\"')\n",
    "print('Tokenized Comment: {}'.format(sample_token))\n",
    "print('Tokenized Comment After Removing Stop Words: {}'.format(sample_token_no_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some NLP preprocessing techniques defined, let's use them in practice on a real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Reviews for Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the analysis to clean up the reviews so that they can be easily tokenized. There are two parts of the review that need to be cleaned:\n",
    "\n",
    "1. Remove punctuation\n",
    "2. Expand contraction words (i.e. don't --> do not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove punctuation, we define a function, ```remove_punctuation```, that utilizes the ```string``` library to look for and remove any symbols related to punctuation, with the exception of apostrophes (needed for expanding contraction words). The ```string``` library is a default library in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    nopunc = []\n",
    "    # Looks for and removes punctuations from string\n",
    "    for char in text:\n",
    "        if char == '’' or char == \"'\":\n",
    "            nopunc.append(\"'\")\n",
    "        # Punctuation in string.punctuation: '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        elif char in string.punctuation and char != \"'\":\n",
    "            nopunc.append(' ')\n",
    "        else:\n",
    "            nopunc.append(char)\n",
    "    # After removing punctuation, join string back together\n",
    "    output = ''.join(nopunc)\n",
    "    \n",
    "    return output\n",
    "\n",
    "sample_sentence_one = 'here: a sentence - alibeit convoluted - with lots(!) of random $%^ punctuation?'\n",
    "\n",
    "print('Raw Comment: {}'.format(sample_sentence_one))\n",
    "print('Cleaned Comment: {}'.format(remove_punctuation(sample_sentence_one)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanding Contraction Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To expand contraction words, we must first define a mapping of a contraction word to its expanded form. Fortunately, someone on the internet has already done this (https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python). To avoid adding extra space to this notebook, the dictionary of contraction word mappings was placed in ```contractions_v2.py```, then called into this notebook using ```import```. The code cell below shows how the contraction word mappings are brought into the notebook, as well as a print of 10 terms within the contraction word mapping dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dictionary of english language contractions\n",
    "from contractions_v2 import get_word_contractions\n",
    "contract_list = get_word_contractions()\n",
    "\n",
    "print('Example of items in the contraction word list dictionary:')\n",
    "dict(list(contract_list.items())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the mapping has been added, we can define a function, ```expand_contraction_words```, that looks at each word in the review and expands a contraction word if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contraction_words(lst, contraction_list):\n",
    "    final_output = []\n",
    "    for item in lst:\n",
    "        if item.lower() in contraction_list.keys():\n",
    "            final_output.append(contraction_list[item.lower()])\n",
    "        else:\n",
    "            final_output.append(item)\n",
    "            \n",
    "    return final_output\n",
    "            \n",
    "print('Example input/output of expand_contraction_words function:')\n",
    "print('Input: {}'.format([\"we\", \"aren't\", \"he\", \"didn't\", \"she\", \"wouldn't\"]))\n",
    "print('Output: {}'.format(expand_contraction_words([\"we\", \"aren't\", \"he\", \"didn't\", \"she\", \"wouldn't\"], contract_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the ```remove_punctuation``` and ```expand_contraction_words``` functions together, we can define a new function, ```clean_review``` which takes a review with punctuations and contraction words and returns a review with punctuation removed, and contraction words expanded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that removes punctuation and expands contraction words\n",
    "def clean_review(text, contraction_list):\n",
    "    # Remove Punctuation\n",
    "    nopunc = remove_punctuation(text).split(' ')\n",
    "    \n",
    "    # Expand contraction words\n",
    "    final_output = expand_contraction_words(nopunc, contraction_list)\n",
    "            \n",
    "    return ' '.join(final_output).lower()\n",
    "\n",
    "sample_sentence_two = \"WHOA! I DON'T THINK SENTENCE(?) is cleaned*...or maybe it's clean NOW\"\n",
    "\n",
    "print('Here is an example of a cleaned comment:')\n",
    "print('Raw Comment: {}'.format(sample_sentence_two))\n",
    "print('Cleaned Comment: {}'.format(clean_review(sample_sentence_two, contract_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing, Stemming/Lemmatizing, and Removing Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the reviews in a format that can be used for text analysis. This step involves removing stopwords, tokenizing the review, and stemming / lemmatizing words within the review. The function below does all of the above steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, stemmer=None):\n",
    "    stop_words = set([i for i in nltk.corpus.stopwords.words('english') if i not in ['no','not']])\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    \n",
    "    if stemmer == None:\n",
    "        return [word for word in tokenized_text if word not in stop_words]\n",
    "    elif stemmer == 'Porter':\n",
    "        return [porter.stem(word) for word in tokenized_text if word not in stop_words]\n",
    "    elif stemmer == 'Lancaster':\n",
    "        return [lancaster.stem(word) for word in tokenized_text if word not in stop_words]\n",
    "    elif stemmer == 'Lemmatize':\n",
    "        return [lemmatizer.lemmatize(word) for word in tokenized_text if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below applies the above function to the dataset, and puts all reviews together in list of lists (i.e. a corpus of reviews). As an example, there is a variable defined below for each potential stemming/lemmatizing scenario discussed (No Stemming/Lemmatization, Porter, Lancaster, and Lemmatize). However, **moving forward in this notebook, we will use the scenario where all reviews are Lemmatized (i.e. corp_lemma)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_norm = [tokenize(clean_review(comment, contract_list)) for comment in data['reviews.text']]\n",
    "corp_lancaster = [tokenize(clean_review(comment, contract_list), stemmer='Lancaster') for comment in data['reviews.text']]\n",
    "corp_lemma = [tokenize(clean_review(comment, contract_list), stemmer='Lemmatize') for comment in data['reviews.text']]\n",
    "corp_porter = [tokenize(clean_review(comment, contract_list), stemmer='Porter') for comment in data['reviews.text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our comments are tokenized, we can start to use more functionality within ```NLTK```. The first function to show is ```FreqDist```, which can allow you to see the most common words within the reviews. To do this, we must first flatten out the corpus of reviews (go from list of lists to one long list with all the words from all the reviews). Then we can use ```FreqDist``` and ```plot``` to show the words with the highest frequency. In addition to the plot, we can do some additional processing to get the word frequencies in a tabular (DataFrame) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLTK FreqDist to get the most common words in the review\n",
    "word_frequency = nltk.FreqDist([item for sublist in corp_lemma for item in sublist])\n",
    "word_frequency.plot(10)\n",
    "\n",
    "word_frequency_df = pd.DataFrame.from_dict(word_frequency, orient='index', columns=['frequency'])\n",
    "word_frequency_df.reset_index(level=0, inplace=True)\n",
    "word_frequency_df = word_frequency_df.rename(columns={'index': 'word'})\n",
    "word_frequency_df.sort_values(by=['frequency'], ascending=False).iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Grams refers to the  “contiguous sequence of N items from a given sample of text or speech”. The frequency distribution shown above is an example of **unigrams** or 1-grams. ```NLTK``` allows you to look at as many continuous sequence lengths as desired given tokenized text data and the function ```ngrams```. Below are examples of what 2-grams (or **bigrams**) and 3-grams (or **tri-grams**) look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "print('Tokenized Comment: {}'.format(corp_lemma[0]))\n",
    "\n",
    "corp_lemma_bigrams = [tuple(ngrams(comment, 2)) for comment in corp_lemma if len(comment) > 0]\n",
    "print('Example Bigrams: {}'.format(corp_lemma_bigrams[0]))\n",
    "\n",
    "corp_lemma_trigrams = [tuple(ngrams(comment, 3)) for comment in corp_lemma if len(comment) > 0]\n",
    "print('Example Trigrams: {}'.format(corp_lemma_trigrams[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also still use the ```FreqDist``` function in ```NLTK``` to see the frequency of particular bigrams or trigrams within our corpus of text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bigrams Frequency Distribution')\n",
    "nltk.FreqDist([item for sublist in corp_lemma_bigrams for item in sublist]).plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Trigrams Frequency Distribution')\n",
    "nltk.FreqDist([item for sublist in corp_lemma_trigrams for item in sublist]).plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One variation on N-grams are **skipgrams**. Skipgrams are similar to N-grams in that you are looking for items in your text that are of the same number of words (i.e. for bigrams, all items were two word pairs). However, the key difference is that you are allowed to \"skip\" words that appear in between. Skipgrams gets all N-word combinations of a review given the allowed number of words you can skip. Note that for skipgrams you need at least a two-gram (because there are no words between a single word!). Here is an example to better illustrate what skipgrams does.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import skipgrams\n",
    "\n",
    "sample_text = ['hello', 'welcome', 'informs', 'meetup', 'something', 'nltk']\n",
    "bigram_sample = list(ngrams(sample_text, 2))\n",
    "bigram_skip_sample = list(skipgrams(sample_text, 2, 2))\n",
    "\n",
    "print('Tokenized comment: {}'.format(sample_text))\n",
    "print('Example Bigrams: {}'.format(bigram_sample))\n",
    "print('Example Trigrams with 2 word skip: {}'.format(bigram_skip_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the number of \"grams\" in your list will grow exponentially as your N or number of allowed skips between words increases. Here is an example from our dataset when looking at trigrams and allowed 3 word skip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenized Comment (length {}): {}'.format(len(corp_lemma[0]), corp_lemma[0]))\n",
    "print('Example Trigrams (length {}): {}'.format(len(corp_lemma_trigrams[0]), corp_lemma_trigrams[0]))\n",
    "\n",
    "corp_lemma_skipgrams = [tuple(skipgrams(comment, 3, 3)) for comment in corp_lemma]\n",
    "print('\\nExample Trigrams with allowed 3 word skip (length {}):\\n {}'.format(len(corp_lemma_skipgrams[0]), corp_lemma_skipgrams[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with N-grams, you can also use ```FreqDist``` with skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist([item for sublist in corp_lemma_skipgrams for item in sublist]).plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the definitions and data processing work, we can finally get to our model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For topic modelling, we can use the package ```gensim```, which does topic modelling using a technique called Latent Dirichlet Allocation (https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). At a high level, Latent Dirichlet Allocation is a statistical model that assigns probabilities of words being in a topic based on words in the review itself and words in all of the reviews. \n",
    "\n",
    "To get started with topic modelling, we first have to create corpus objects within Gensim so that it can run the analysis. To do this, we use the one of the corpus list of lists we defined earlier in the notebook. For the purposes of the analysis, we will use the ```corp_lemma``` variable as our corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(corp_norm)\n",
    "corpus = [dictionary.doc2bow(text) for text in corp_norm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will save the corpus in a format that can be used for a later visualization of results. To do this, we use the ```pickle``` package in python, which allows you to serialize or deserialize a Python object for later use (for more information on pickle, see the appendix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the number of topics we want the model to output, set parameters for the model, and run the model using the corpus and dictionary variables defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 10\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS,\n",
    "                                           id2word=dictionary, passes=20, random_state=12)\n",
    "ldamodel.save('model.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there is no set way to decide how to choose the number of topics. This part of the analysis is where domain knowledge and trial-and-error come into play. One way in ```gensim``` to interpret the topic modelling results is to print out the highest probability words within each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another handy way to interpret the topic modelling results is by utilizing ```pyLDAvis```, a topic modelling visualization package that is supplemental to ```gensim```.  The ```pyLDAvis``` package takes the results from your gensim model to create the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pyLDA to visualize topics\n",
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model.gensim')\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a visualization of the topic modeling results. The visualization has several helpful features to understand the topics generated by the model.\n",
    "\n",
    "1. On the left side, there is a distance map that shows how close in similarity each topic is relative to one another, as well as the size of the topic relative to the entire corpus of comments\n",
    "2. Based on the topic you hover over/select on the left side, the right side updates with a bar chart of the top 30 words within that topic. The bar chart has a blue bar that shows the overall term frequency, and a red bar that shows the estimated term frequency within the selected topic. \n",
    "    a. Note that when no topic is selected, the bar chart on the right only shows overall term frequencies\n",
    "3. On the top right, there is a slider that can be used to adjust how much weight the visualization places on how relevent a given word is to the selected topic vs. the overall term frequency. A lower value of $\\lambda$ means that more weight is placed on the topic relevancy of words. For example, if a word in the topic only appears in the selected topic, it will be given more weight over a word that appears with a high frequency in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Comments to Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the visualization, the topics generated become a lot more clear. Now, to finish off the algorithm, we want to label the topics and map them to each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of labelling topics for the Hotel sample dataset\n",
    "\n",
    "# Define your topic categories based on topic model analysis above\n",
    "topic_cats = {0:'Spanish Reviews',\n",
    "              1:'Dirty/Noisy Room',\n",
    "              2:'Location',\n",
    "              3:'Room Utilities',\n",
    "              4:'Good Experience',\n",
    "              5:'Anaheim Stadium',\n",
    "              6:'Front Desk',\n",
    "              7:'Thank You',\n",
    "              8:'Hotel Stay',\n",
    "              9:'Scenery'}\n",
    "\n",
    "\n",
    "# Get topic numbers from LDA model for each review\n",
    "corpus_topics = list(ldamodel.get_document_topics(corpus))\n",
    "\n",
    "\n",
    "# Map topic label numbers to topic category names\n",
    "topic_labels = []\n",
    "for comment in corpus_topics:\n",
    "    probs = {k:v for k, v in comment}\n",
    "    best_topic = max(probs, key=lambda key: probs[key])\n",
    "    topic_labels.append(topic_cats[best_topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add a column for the topic labels to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add topic label column to dataset\n",
    "data['topic'] = topic_labels\n",
    "\n",
    "pd.set_option('max_colwidth', 120)\n",
    "display(data[['reviews.text', 'topic']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Based Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While using an unsupervised approach such as topic modelling can be effective in some business cases, other business cases may required a more targeted approach. Perhaps the business user has specific categories in mind that they would like to explore in the data. In this case, a more brute force rule based application may be more desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section will cover some basic strategies for rule based tagging. Namely, single tagging and bigram tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Tagging\n",
    "\n",
    "*How does it work?* Compare each individual word with a dictionary of words associated with a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy Example\n",
    "comment = 'Love the product. Communication was great and delivery was fast! Thanks!'\n",
    "\n",
    "# Tokenized comment from cell above ('Love the shoes. Communication was great and delivery was fast! Thanks!')\n",
    "tokenized_text = tokenize(comment)\n",
    "\n",
    "# Words associated with the category, 'Fast Delivery' (NOTE: not full list)\n",
    "fast_delivery_words = ['fast','quick','speedy','prompt']\n",
    "\n",
    "# Loop through each word in tokenized comment. If word is in fast_delivery_words, then tag\n",
    "for word in tokenized_text:\n",
    "    print('Is \"{}\" in {}??'.format(word, fast_delivery_words), end='\\t----> ', flush=True)\n",
    "    if word in fast_delivery_words:\n",
    "        print('Yes! Tag this comment as \"Fast Delivery\".')\n",
    "        break\n",
    "    else:\n",
    "        print('No, keep going')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Tagging\n",
    "\n",
    "*How does it work?* Same as with single tagging but in this case we are comparing *pairs* of words with a word dictionary of *paired* words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy Example - Sad hat guy (real comment)\n",
    "\n",
    "comment = '''I never got my hat! It said t was delivered at 6am on a Saturday but I would’ve been home \n",
    "then and I never heard a knock (also who delivers packages at 6am?). I want my hat :( I am sad without \n",
    "my hat :( Please send a new hat to my work address so I can have a hat :('''\n",
    "\n",
    "# Tokenize\n",
    "tokenized_text = tokenize(comment)\n",
    "\n",
    "# Extract pairs of words. Can be done using nltk.bigrams (text must be tokenized first)\n",
    "text_bigrams = nltk.bigrams(tokenized_text)\n",
    "\n",
    "print('Bigrams: {}\\n'.format(list(text_bigrams)))\n",
    "\n",
    "# Words associated with the category, 'Not Received' (NOTE: not full list)\n",
    "not_received_pairs = (('not', 'received'), ('never', 'got'))\n",
    "\n",
    "print('Loop through comment:')\n",
    "# Loop through each word pair. If word pair (or it's reversed pair) is in not_received_words, then tag\n",
    "for bigram in nltk.bigrams(tokenized_text):\n",
    "    print('Is \"{}\" in {}??'.format(bigram, not_received_pairs), end='\\t----> ', flush=True)\n",
    "    # Need to check reverse of pair as well because order matters when checking pairs\n",
    "    if bigram in not_received_pairs or tuple(reversed(bigram)) in not_received_pairs:\n",
    "        print('Yes! Tag this comment as \"Not Received\".')\n",
    "        break\n",
    "    else:\n",
    "        print('No, keep going')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negation and Secondary Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging by individual words and pairs of words can be effective for basic tagging, but there are some scenarios where this approach can misclassify comments due to words that reverse sentence meaning or because there isn't enough context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But wait, NO (checking for negation terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you don't want to tag a comment, even though the word appears in the word dictionary. The reason is that the customer may be expressing a reverse sentiment from what the category implies.\n",
    "\n",
    "**EXAMPLE:** ```'The packaging and shipping was good as usual since there was NO damages to the box and the product came on time.'```\n",
    "\n",
    "In the above case, you do NOT want to tag this as damage to the box. So, we must in some cases check the surrounding words before tagging.\n",
    "\n",
    "First, we define a way to look at words surrounding a word that appears in the word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- INPUT:\n",
    "    1. tokenized_text - Text in tokenized form (can use token_simple function or any other tokenizer)\n",
    "    2. index - Index of the comment word that appears in the word dictionary (the word that you are checking around \n",
    "    to see if there is a negation term)\n",
    "    3. check_type - Where you want to be checking. Do you want to look at the words that came BEFORE the comment \n",
    "    word, AFTER the comment word, or BOTH?\n",
    "    4. ctr - HOW MANY words nearest to the comment word you want to look at (default is 3)\n",
    "\n",
    "- OUTPUT:\n",
    "    Tokenized list showing only words you want around the comment word\n",
    "'''\n",
    "def surrounding_words(tokenized_text, index, check_type='backward', ctr=None):\n",
    "\n",
    "    # Check how many words before and/or after the word that you want to check\n",
    "    if ctr is None:\n",
    "        ctr = 3\n",
    "    else:\n",
    "        assert type(ctr) is int, 'ctr is not an int'\n",
    "    \n",
    "    # Get words that occur BEFORE the given index \n",
    "    # (number of words equal to ctr OR the number available words before index)\n",
    "    if check_type == 'backward':\n",
    "        if index - ctr < 0:\n",
    "            word_check = tokenized_text[0:index]\n",
    "            if len(word_check) == 0:\n",
    "                return tokenized_text\n",
    "        else:\n",
    "            word_check = tokenized_text[index-ctr:index]\n",
    "    \n",
    "    # Get words that occur AFTER the given index \n",
    "    # (number of words equal to ctr OR the number available words after index)         \n",
    "    elif check_type == 'forward':\n",
    "        if index + ctr >= len(tokenized_text):\n",
    "            word_check = tokenized_text[index:len(tokenized_text)]\n",
    "            if len(word_check) == 0:\n",
    "                return tokenized_text\n",
    "        else:\n",
    "            word_check = tokenized_text[index:index+ctr+1]\n",
    "    \n",
    "    # Get words that occur before AND after the given index \n",
    "    # (number of words before and after equal to ctr OR the number available words before and after index)\n",
    "    elif check_type == 'both':\n",
    "        if index - ctr < 0 and index + ctr < len(tokenized_text):\n",
    "            word_check = tokenized_text[0:index+ctr+1]\n",
    "        elif index - ctr > 0 and index + ctr >= len(tokenized_text):\n",
    "            word_check = tokenized_text[index-ctr:len(tokenized_text)]\n",
    "        elif index - ctr < 0 and index + ctr >= len(tokenized_text):\n",
    "            word_check = tokenized_text[:]\n",
    "        else:\n",
    "            word_check = tokenized_text[index-ctr:index+ctr+1]\n",
    "        if len(word_check) == 0:\n",
    "            return tokenized_text\n",
    "\n",
    "    # Return list of surrounding words\n",
    "    return word_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example comment\n",
    "comment = 'The packaging and shipping was good as usual since there was NO damages to the box and the product came on time.'\n",
    "\n",
    "# Tokenize\n",
    "tokenized_text = tokenize(comment)\n",
    "\n",
    "# Let's say we are looking for comments refering to damage, and we find an occurance of damage at a particular index\n",
    "damage_index = tokenized_text.index('damages')\n",
    "\n",
    "# Now, we can use the surrounding_words function to find the words surrounding the word 'damages'\n",
    "print('Here are the words surrounding the occurance of the word \"damages\":')\n",
    "print(surrounding_words(tokenized_text, damage_index, check_type='both'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we use this to determine if we should tag a comment or not? Basically, we need to check the surrounding words against a second word dictionary containing negation terms. In addition, we need to integrate this check into the previous steps. Below is an example of how you would combine single tagging with negation terms: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example comment\n",
    "comment = 'The packaging and shipping was good as usual since there was no damages to the box and the product came on time.'\n",
    "\n",
    "# Tokenize\n",
    "tokenized_text = tokenize(comment)\n",
    "\n",
    "# Words associated with the category, 'Damage' (NOTE: not full list)\n",
    "damage_words = ['damage','damages']\n",
    "\n",
    "# We need a second dictionary of negation terms to check against (NOTE: not full list)\n",
    "negation_words = ['no', 'not', 'wasnt']\n",
    "\n",
    "# Define a flag to indicate if negation term found\n",
    "flg = 0\n",
    "\n",
    "# Loop through each word in tokenized comment. \n",
    "# We use enumerate here to keep track of which index we are on in the tokenized comment\n",
    "for index, word in enumerate(tokenized_text):\n",
    "    if flg == 1:\n",
    "        break\n",
    "    print('Is \"{}\" in {}??'.format(word, damage_words), end='\\t----> ', flush=True)\n",
    "    if word in damage_words:\n",
    "        print('Yes! But wait, lets check if there is a negation term')\n",
    "        # Define the surrounding words space\n",
    "        surrounding = surrounding_words(tokenized_text, index, check_type='both')\n",
    "        print('\\nsurrounding words: {}\\n'.format(surrounding))\n",
    "        for w in surrounding:\n",
    "            print('Is \"{}\" in {}??'.format(w, negation_words), end='\\t----> ', flush=True)\n",
    "            if w in negation_words:\n",
    "                print('Oh no! A negation term! DO NOT TAG')\n",
    "                flg += 1\n",
    "                break\n",
    "            else:\n",
    "                print('Still okay, no negations')\n",
    "    else:\n",
    "        print('No, keep going')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But wait, YES (using surrounding words to confirm a tag)\n",
    "\n",
    "Sometimes, instead of wanting to NOT tag a comment where there is a negation term nearby, you want to tag ONLY WHEN there is a secondary term near the first term.\n",
    "\n",
    "For example, say you want to tag a comment as \"damage box\" and you have the following comment:\n",
    "\n",
    "```my package showed up as if ace ventura was the delivery guy. my package showed up as if the UPS guy kicked a field goal with the box before he dropped it off, the box is completely wrecked, and i usually like to keep them. luckily, the product itself is fine, but i’m very disappointed with how they showed up```\n",
    "\n",
    "(*Note:* Real Comment)\n",
    "\n",
    "In this comment, we know that the comment has to do with damage, because on the second line it says: \"...the shoe box is completely **wrecked**...\", but we can't tag it as damage to the box without getting the second key word in the phrase: \"...the **box** is completely wrecked\". Fortunately, we can use our surrounding words function to get the second key word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example comment\n",
    "comment = '''\n",
    "The box is completely wrecked, and i usually like to keep them. luckily, the product itself\n",
    "is fine, but i’m very disappointed with how they showed up\n",
    "'''\n",
    "\n",
    "# Tokenize\n",
    "tokenized_text = tokenize(comment)\n",
    "\n",
    "# Words associated with the category, 'Damage' (NOTE: not full list)\n",
    "damage_words = ['damage','damages', 'wrecked']\n",
    "\n",
    "# Second dictionary of words associated with boxes (NOTE: not full list)\n",
    "package_words = ['box', 'package']\n",
    "\n",
    "# Define a flag to indicate if second term found\n",
    "flg = 0\n",
    "\n",
    "# Loop through each word in tokenized comment. \n",
    "# We use enumerate here to keep track of which index we are on in the tokenized comment\n",
    "for index, word in enumerate(tokenized_text):\n",
    "    if flg == 1:\n",
    "        break\n",
    "    print('Is \"{}\" in {}??'.format(word, damage_words), end='\\t----> ', flush=True)\n",
    "    if word in damage_words:\n",
    "        print('Yes! But wait, lets check if we have the second term')\n",
    "        # Define the surrounding words space\n",
    "        surrounding = surrounding_words(tokenized_text, index, check_type='both', ctr=5)\n",
    "        print('\\nsurrounding words: {}\\n'.format(surrounding))\n",
    "        for w in surrounding:\n",
    "            print('Is \"{}\" in {}??'.format(w, package_words), end='\\t----> ', flush=True)\n",
    "            if w in package_words:\n",
    "                print('Yes! A second term! TAG TAG TAG')\n",
    "                flg += 1\n",
    "                break\n",
    "            else:\n",
    "                print('Still no second term...')\n",
    "    else:\n",
    "        print('No, keep going')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above tagging approaches, we can mix and match to further improve our tagging accuracy:\n",
    "1. Single tag, no negation, no second term\n",
    "2. Single tag, check negation (single words), no second term\n",
    "3. Single tag, no negation, check second term (single words)\n",
    "4. Single tag, check negation (bigrams), no second term\n",
    "5. Single tag, no negation, check second term (bigrams)\n",
    "6. Bigram tag, no negation, no second term\n",
    "7. Bigram tag, check negation (single words), no second term\n",
    "8. Bigram tag, no negation, check second term (single words)\n",
    "9. Bigram tag, check negation (bigrams), no second term\n",
    "10. Bigram tag, no negation, check second term (bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function below allows you to tag your comments based on one of the 10 scenarios highlighted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_data(df, review_col, tag_col, word_list, check=False, check_type='backward', ctr=3, terms=None, negation=False):\n",
    "    result = df.copy().reset_index()\n",
    "    indexes = []\n",
    "    for index, comment in enumerate(result[review_col]):\n",
    "        # Ignore surveys that don't have a comment\n",
    "        if type(comment) == float:\n",
    "            continue\n",
    "\n",
    "        # Tokenize comment\n",
    "        tokenized_comment = tokenize(comment)\n",
    "\n",
    "        # One word check if word dictionary is list (scenarios 1-5)\n",
    "        if type(word_list) == list:\n",
    "            if check == False:\n",
    "                if any(x in tokenized_comment for x in word_list):\n",
    "                    indexes.append(index)\n",
    "            elif check == True:\n",
    "                for item, word in enumerate(tokenized_comment):\n",
    "                    if word in word_list:\n",
    "                        if type(terms) == list:\n",
    "                            if any(x in surrounding_words(tokenized_comment, item, \n",
    "                                                          check_type=check_type, ctr=ctr) for x in terms):\n",
    "                                if negation == False:\n",
    "                                    indexes.append(index)\n",
    "                                    break\n",
    "                                elif negation == True:\n",
    "                                    break\n",
    "                            else:\n",
    "                                if negation == False:\n",
    "                                    break\n",
    "                                if negation == True:\n",
    "                                    indexes.append(index)\n",
    "                                    break\n",
    "                        elif type(terms) == tuple:\n",
    "                            surrounding = surrounding_words(tokenized_comment, item, check_type=check_type, ctr=ctr)\n",
    "                            for ind, bigram in enumerate(nltk.bigrams(surrounding)):\n",
    "                                if bigram in terms or tuple(reversed(bigram)) in terms:\n",
    "                                    if negation == False:\n",
    "                                        indexes.append(index)\n",
    "                                        break\n",
    "                                    elif negation == True:\n",
    "                                        break\n",
    "                                else:\n",
    "                                    if ind == (len(list(nltk.bigrams(surrounding))) - 1):\n",
    "                                        if negation == False:\n",
    "                                            break\n",
    "                                        elif negation == True:\n",
    "                                            indexes.append(index)\n",
    "                                            break\n",
    "                                    else:\n",
    "                                        continue  \n",
    "        \n",
    "        # Bigram check if word dictionary is tuple (scenarios 6-10)\n",
    "        elif type(word_list) == tuple:\n",
    "            if check == False:\n",
    "                for word in nltk.bigrams(tokenized_comment):\n",
    "                    if word in word_list or tuple(reversed(word)) in word_list:\n",
    "                        indexes.append(index)\n",
    "                        break\n",
    "            elif check == True:\n",
    "                for item, word in enumerate(nltk.bigrams(tokenized_comment)):\n",
    "                    if word in word_list or tuple(reversed(word)) in word_list:\n",
    "                        if type(terms) == list:\n",
    "                            if any(x in surrounding_words(tokenized_comment, item,\n",
    "                                                          check_type=check_type, ctr=ctr) for x in terms):\n",
    "                                if negation == False:\n",
    "                                    indexes.append(index)\n",
    "                                    break\n",
    "                                elif negation == True:\n",
    "                                    break\n",
    "                            else:\n",
    "                                if negation == False:\n",
    "                                    break\n",
    "                                if negation == True:\n",
    "                                    indexes.append(index)\n",
    "                                    break\n",
    "                        if type(terms) == tuple:\n",
    "                            surrounding = surrounding_words(tokenized_comment, item, check_type=check_type, ctr=ctr)\n",
    "                            for ind, bigram in enumerate(nltk.bigrams(surrounding)):\n",
    "                                if bigram in terms or tuple(reversed(bigram)) in terms:\n",
    "                                    if negation == False:\n",
    "                                        indexes.append(index)\n",
    "                                        break\n",
    "                                    elif negation == True:\n",
    "                                        break\n",
    "                                else:\n",
    "                                    if ind == (len(list(nltk.bigrams(surrounding))) - 1):\n",
    "                                        if negation == False:\n",
    "                                            break\n",
    "                                        elif negation == True:\n",
    "                                            indexes.append(index)\n",
    "                                            break\n",
    "                                    else:\n",
    "                                        continue\n",
    "    \n",
    "    tag_rows = result[result.index.isin(indexes)]\n",
    "    tag_row_comments = tag_rows[review_col].unique()\n",
    "    \n",
    "    result[tag_col] = result.apply(lambda x: 1 if x[review_col] in tag_row_comments else 0, axis=1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To close, here are some examples of rule based tagging in action. Here, we have three examples: *good experience (with negation check)*, *food*, and *front desk*. There are two steps to the analysis:\n",
    "\n",
    "1. Define the dictionary of terms you want to match to\n",
    "2. Run the tag data function, applying the appropriate tag scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Define Dictionary of terms\n",
    "\n",
    "# Good Experience\n",
    "good_experience = ['great', 'nice', 'good', 'awesome', 'wonderful', 'amazing', 'perfect']\n",
    "negation = ['no', 'not', 'wasnt']\n",
    "\n",
    "# Food\n",
    "food = ['food', 'breakfast', 'lunch', 'dinner']\n",
    "\n",
    "# Front Desk\n",
    "front_desk = ('front', 'desk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Run Tag Data Function with appropriate tag scenario\n",
    "\n",
    "good_experience_tag = tag_data(data, 'reviews.text', 'good experience', good_experience, \n",
    "                               check=True, check_type='backward', ctr=3, terms=negation, negation=True)\n",
    "\n",
    "food_tag = tag_data(data, 'reviews.text', 'food', food)\n",
    "\n",
    "front_desk_tag = tag_data(data, 'reviews.text', 'front_desk', front_desk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of comments tagged as good experience\n",
    "\n",
    "good_experience_tag[good_experience_tag['good experience'] == 1][['reviews.text', 'good experience']].iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for going through this tutorial notebook on text analytics in Python! If you have any questions on any of the material covered, please feel free to reach out at **michaeljcho29@gmail.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "\n",
    "NLTK Online Textbook\n",
    "1. http://www.nltk.org/book/\n",
    "\n",
    "Stemming and Lemmatization \n",
    "1. https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8\n",
    "2. https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "3. https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "\n",
    "Topic Modelling with Gensm\n",
    "1. https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "\n",
    "Pickle\n",
    "1. https://docs.python.org/3/library/pickle.html\n",
    "2. https://www.geeksforgeeks.org/understanding-python-pickling-example/\n",
    "\n",
    "pyLDAvis\n",
    "1. https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt_test",
   "language": "python",
   "name": "virt_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
